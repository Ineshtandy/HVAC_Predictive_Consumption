{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693209af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hack it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375888ca",
   "metadata": {},
   "source": [
    "# Setting up the Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54708a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  \n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_checker import check_env  \n",
    "\n",
    "class SensorBasedThermalEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Updated HVAC environment:\n",
    "    - All temperatures in Fahrenheit\n",
    "    - T_in between 65째F and 75째F\n",
    "    - Simulated sensor-like indoor temp readings\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        super(SensorBasedThermalEnv, self).__init__()\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_time_index = len(self.df) - 9  # 8 steps ahead needed for future\n",
    "\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Observation: [T_in, T_out, T_out_future, Humid, Wind_sp, Time_step]\n",
    "        low_obs = np.array([60.0, -10.0, -10.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        high_obs = np.array([90.0, 120.0, 120.0, 100.0, 50.0, 7.0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low_obs, high=high_obs, dtype=np.float32)\n",
    "\n",
    "        self.energy_rate = 1.0\n",
    "        self.episode_length = 8  # 2 hours (assuming 15-min steps)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        Reset environment\n",
    "        \"\"\"\n",
    "        self.start_idx = np.random.randint(0, self.max_time_index)\n",
    "        self.current_idx = self.start_idx\n",
    "\n",
    "        self.T_in = np.random.uniform(65.0, 75.0)  # Indoor temp in F\n",
    "        self.T_target = self.T_in\n",
    "        self.time_step = 0\n",
    "\n",
    "        state = self._get_obs()\n",
    "        return state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Step the environment\n",
    "        \"\"\"\n",
    "        HVAC_power = np.clip(action[0], 0.0, 1.0)\n",
    "        energy_used = HVAC_power * self.energy_rate\n",
    "\n",
    "        # Current weather readings\n",
    "        T_out = self.df.loc[self.current_idx, 'temp']  # Already in Fahrenheit\n",
    "        humid = self.df.loc[self.current_idx, 'humid']\n",
    "        wind_sp = self.df.loc[self.current_idx, 'wind_sp']\n",
    "\n",
    "        # Sensor-based T_in_next simulation:\n",
    "        # Random small noise, outdoor influence, HVAC effect independently applied\n",
    "        temp_drift = np.random.uniform(0.05, 0.2) * (T_out - self.T_in) / 10  # Outdoor drift\n",
    "        wind_cooling = np.random.uniform(0.01, 0.05) * (-wind_sp) / 10        # Wind cooling\n",
    "        hvac_effect = HVAC_power * np.random.uniform(0.5, 1.0) * (-5.0)       # HVAC cooling only\n",
    "\n",
    "        sensor_noise = np.random.normal(0, 0.2)  # Small random sensor noise\n",
    "\n",
    "        T_in_next = self.T_in + temp_drift + wind_cooling + hvac_effect + sensor_noise\n",
    "        T_in_next = np.clip(T_in_next, 60.0, 90.0)  # Reasonable indoor temp range\n",
    "\n",
    "        # Calculate reward\n",
    "        comfort_penalty = abs(T_in_next - self.T_target)\n",
    "        reward = -comfort_penalty - 0.1 * energy_used\n",
    "\n",
    "        # Update for next step\n",
    "        self.T_in = T_in_next\n",
    "        self.current_idx += 1\n",
    "        self.time_step += 1\n",
    "        done = (self.time_step >= self.episode_length)\n",
    "\n",
    "        next_state = self._get_obs()\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Current observation\n",
    "        \"\"\"\n",
    "        T_out = self.df.loc[self.current_idx, 'temp']\n",
    "        humid = self.df.loc[self.current_idx, 'humid']\n",
    "        wind_sp = self.df.loc[self.current_idx, 'wind_sp']\n",
    "\n",
    "        T_out_future = self.df.loc[self.current_idx + 8, 'temp']  # 2 hours ahead\n",
    "\n",
    "        state = np.array([self.T_in, T_out, T_out_future, humid, wind_sp, self.time_step], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step {self.time_step}: T_in={self.T_in:.2f}째F, T_out={self.df.loc[self.current_idx, 'temp']:.1f}째F, Humid={self.df.loc[self.current_idx, 'humid']:.1f}%, Wind={self.df.loc[self.current_idx, 'wind_sp']:.1f} m/s\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your updated environment class\n",
    "# (Assuming you've defined the class SensorBasedThermalEnv from earlier.)\n",
    "\n",
    "env = SensorBasedThermalEnv(df)\n",
    "\n",
    "# Always good: check if environment is compatible\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b739f",
   "metadata": {},
   "source": [
    "# Creating the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc399b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SAC model\n",
    "model = SAC(\n",
    "    policy=\"MlpPolicy\",  # simple multilayer perceptron policy\n",
    "    env=env,\n",
    "    device = 'mps',\n",
    "    verbose=1,            # shows training progress\n",
    "    learning_rate=3e-4,   # how fast agent learns\n",
    "    buffer_size=100000,   # experience replay buffer size\n",
    "    batch_size=64,        # mini-batch size for learning\n",
    "    tau=0.005,            # target smoothing coefficient\n",
    "    gamma=0.99,           # reward discount factor\n",
    "    train_freq=(1, \"step\"),  # learn every step\n",
    "    gradient_steps=1,     # gradient update steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ae99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "model.learn(total_timesteps=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_mode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
